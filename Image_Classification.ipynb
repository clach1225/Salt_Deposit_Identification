{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import skimage\n",
    "from skimage import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Concatenate, Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras\n",
    "\n",
    "import keras.utils as np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the Data to be Trained and Labeled.\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "X_images = []\n",
    "y_images = []\n",
    "\n",
    "#Data Paths\n",
    "train_image_path = r\"C:\\Users\\buckf\\Documents\\Practicum_2\\Data\\train\\images\"\n",
    "train_mask_path = r\"C:\\Users\\buckf\\Documents\\Practicum_2\\Data\\train\\masks\"\n",
    "\n",
    "\n",
    "image_list = os.listdir(train_image_path)\n",
    "mask_list = os.listdir(train_mask_path)\n",
    "\n",
    "for image_num in range(len(image_list)):\n",
    "    #Gets the Path to the Mask\n",
    "    certain_image_path = os.path.join(train_image_path, image_list[image_num])\n",
    "    image_img = Image.open(certain_image_path).convert('L')\n",
    "    image_pix = np.array(image_img.getdata()).reshape(image_img.size[0], image_img.size[1], 1)\n",
    "    X.append(image_pix)\n",
    "    X_images.append(image_img)\n",
    "\n",
    "for mask_num in range(len(mask_list)):\n",
    "    #Gets the Path to the Mask\n",
    "    certain_mask_path = os.path.join(train_mask_path, mask_list[mask_num])\n",
    "    mask_img = Image.open(certain_mask_path).convert('L')\n",
    "    mask_pix = np.array(mask_img.getdata()).reshape(mask_img.size[0], mask_img.size[1], 1)\n",
    "    y.append(mask_pix)\n",
    "    y_images.append(mask_img)\n",
    "    \n",
    "X = np.array(X, dtype = 'float32')\n",
    "y = np.array(y, dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=101x101 at 0x1A2BF82C588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGUAAABlCAAAAABxF3ITAAAA5ElEQVR4nO2Zyw7DIAwEl6r//8v00ChpXhJrAlro7hEkj8bGJ1LGIQkATqfrTSjv08kloDKvBjVHoMR7SbmEMVzHohi9uSAsw7rEMJIdM8WUWSihtRR1mYoSGYyqy1SUwGBkXaai8IPRdZmKQg9G2MWUv6awT1nZhZWRdjGFDjd+bRdTRCnUIxN3MYUOM/4KFwKj3jFTTDGFSfny67t0ohS3bACXYpnzD29lvuDDl3Oq/TZOyL9F83r8KGUrmnfFdpiHKHfkJQ3fWN4EGrosAKDDvmSggwuAMXbfFFNMMcUUU67zAeQ5I7HCfT4XAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=101x101 at 0x1A2CA8D4780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This is how data looks. \n",
    "#The top is the what we need to seperate salt from sediment.\n",
    "#The bottom is the mask or label we need to predict. \n",
    "display(X_images[10], y_images[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 101, 101, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 101, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\buckf\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 101, 101, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function keras.backend.tensorflow_backend.clear_session()>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 14779.0265 - acc: 0.2107 - val_loss: 12947.4070 - val_acc: 0.0127\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 2s 815us/step - loss: 13727.0093 - acc: 0.2107 - val_loss: 12800.0740 - val_acc: 0.0127\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 2s 836us/step - loss: 13247.9624 - acc: 0.2106 - val_loss: 12843.5197 - val_acc: 0.0127\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 2s 784us/step - loss: 13061.7404 - acc: 0.2107 - val_loss: 12931.9386 - val_acc: 0.0127\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 2s 786us/step - loss: 12994.1043 - acc: 0.2109 - val_loss: 13006.1712 - val_acc: 0.0127\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 2s 767us/step - loss: 12979.6892 - acc: 0.2106 - val_loss: 13052.1925 - val_acc: 0.0127\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 2s 836us/step - loss: 12972.4190 - acc: 0.2107 - val_loss: 13077.1751 - val_acc: 0.0127\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 2s 786us/step - loss: 12974.1181 - acc: 0.2106 - val_loss: 13098.2523 - val_acc: 0.0127\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 2s 792us/step - loss: 12972.1427 - acc: 0.2109 - val_loss: 13089.7182 - val_acc: 0.0127\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 2s 789us/step - loss: 12970.6477 - acc: 0.2108 - val_loss: 13084.9485 - val_acc: 0.0127\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 2s 830us/step - loss: 12970.1826 - acc: 0.2107 - val_loss: 13076.3483 - val_acc: 0.0127\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 2s 797us/step - loss: 12970.7496 - acc: 0.2108 - val_loss: 13083.0274 - val_acc: 0.0127\n",
      "Epoch 13/20\n",
      "2000/2000 [==============================] - 2s 846us/step - loss: 12971.1512 - acc: 0.2108 - val_loss: 13102.8773 - val_acc: 0.0127\n",
      "Epoch 14/20\n",
      "2000/2000 [==============================] - 2s 792us/step - loss: 12971.7871 - acc: 0.2106 - val_loss: 13099.5444 - val_acc: 0.0127\n",
      "Epoch 15/20\n",
      "2000/2000 [==============================] - 2s 862us/step - loss: 12972.4310 - acc: 0.2108 - val_loss: 13095.7645 - val_acc: 0.0127\n",
      "Epoch 16/20\n",
      "2000/2000 [==============================] - 2s 866us/step - loss: 12970.3438 - acc: 0.2107 - val_loss: 13102.9705 - val_acc: 0.0127\n",
      "Epoch 17/20\n",
      "2000/2000 [==============================] - 2s 762us/step - loss: 12970.1480 - acc: 0.2107 - val_loss: 13097.9523 - val_acc: 0.0127\n",
      "Epoch 18/20\n",
      "2000/2000 [==============================] - 2s 921us/step - loss: 12968.0411 - acc: 0.2107 - val_loss: 13104.5195 - val_acc: 0.0127\n",
      "Epoch 19/20\n",
      "2000/2000 [==============================] - 2s 902us/step - loss: 12970.3400 - acc: 0.2107 - val_loss: 13086.0029 - val_acc: 0.0127\n",
      "Epoch 20/20\n",
      "2000/2000 [==============================] - 2s 846us/step - loss: 12967.9352 - acc: 0.2108 - val_loss: 13097.8508 - val_acc: 0.0126\n",
      "34.060842752456665  seconds\n",
      "CNN Error: 98.74%\n"
     ]
    }
   ],
   "source": [
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "num_classes = y_val.shape[1]\n",
    "\n",
    "def simple_cnn_model():\n",
    "    \n",
    "    # create model\n",
    "### YOUR TURN\n",
    "# Build a model that has 1 convolution layer, 1 max pooling, 1 dense, and output \n",
    "# Use 32 filters with 5x5 size\n",
    "# For max pooling layer, make the layer such that the featuremap size would be the half after the pooling layer\n",
    "# hint: you need to change the argument input_shape to (w,h,1) in the first conv layer\n",
    "# hint: you need Flatten() before the first dense layer\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(1, kernel_size = (1,1), activation = 'relu', input_shape = (X_train.shape[1], X_train.shape[2], 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.compile(loss= keras.losses.mean_squared_error , optimizer= 'adam' , metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "# build the model\n",
    "model = simple_cnn_model()\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "t0 = time.time()\n",
    "log = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=15, verbose=1)\n",
    "t1 = time.time()\n",
    "print(t1-t0, \" seconds\")\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.conv2d_transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 101, 101, 32)      320       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 101, 101, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 101, 101, 1)       289       \n",
      "=================================================================\n",
      "Total params: 9,857\n",
      "Trainable params: 9,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 12687.7165 - acc: 0.0296 - val_loss: 12692.5502 - val_acc: 0.0125\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 12030.5173 - acc: 0.0277 - val_loss: 12340.4955 - val_acc: 0.0130\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 11789.7508 - acc: 0.0294 - val_loss: 12434.7398 - val_acc: 0.0127\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 11587.0349 - acc: 0.0295 - val_loss: 11910.3232 - val_acc: 0.0158\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 11323.5267 - acc: 0.0314 - val_loss: 12277.8340 - val_acc: 0.0175\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 11021.8437 - acc: 0.0332 - val_loss: 11403.6518 - val_acc: 0.0212\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 10727.5242 - acc: 0.0378 - val_loss: 11001.6634 - val_acc: 0.0194\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 10773.8933 - acc: 0.0366 - val_loss: 11749.6565 - val_acc: 0.0168\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 10669.7119 - acc: 0.0355 - val_loss: 10741.6266 - val_acc: 0.0187\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 10513.1826 - acc: 0.0354 - val_loss: 10691.2099 - val_acc: 0.0156\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 10376.5825 - acc: 0.0341 - val_loss: 10610.3157 - val_acc: 0.0146\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 10299.9765 - acc: 0.0330 - val_loss: 10461.1360 - val_acc: 0.0147\n",
      "Epoch 13/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 10239.5545 - acc: 0.0316 - val_loss: 10172.5242 - val_acc: 0.0174\n",
      "Epoch 14/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 10093.9108 - acc: 0.0308 - val_loss: 10057.4194 - val_acc: 0.0148\n",
      "Epoch 15/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 9960.4757 - acc: 0.0304 - val_loss: 10489.1486 - val_acc: 0.0146\n",
      "Epoch 16/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 10213.8732 - acc: 0.0305 - val_loss: 10164.2520 - val_acc: 0.0141\n",
      "Epoch 17/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 10617.7971 - acc: 0.0299 - val_loss: 10335.4653 - val_acc: 0.0137\n",
      "Epoch 18/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 10255.6774 - acc: 0.0272 - val_loss: 10481.7561 - val_acc: 0.0136\n",
      "Epoch 19/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 10186.9385 - acc: 0.0269 - val_loss: 10424.8736 - val_acc: 0.0146\n",
      "Epoch 20/20\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 9956.7799 - acc: 0.0271 - val_loss: 9902.0273 - val_acc: 0.0138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a290467588>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "num_classes = y_val.shape[1]\n",
    "\n",
    "def simple_cnn_model():\n",
    "    \n",
    "    # create model\n",
    "### YOUR TURN\n",
    "# Build a model that has 1 convolution layer, 1 max pooling, 1 dense, and output \n",
    "# Use 32 filters with 5x5 size\n",
    "# For max pooling layer, make the layer such that the featuremap size would be the half after the pooling layer\n",
    "# hint: you need to change the argument input_shape to (w,h,1) in the first conv layer\n",
    "\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size = (3,3),\n",
    "                     activation = 'relu',\n",
    "                     padding = 'same',\n",
    "                     input_shape = (X_train.shape[1], X_train.shape[2], 1)))\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size = (3,3),\n",
    "                     padding = 'same',\n",
    "                     activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    \n",
    "    model.add(Conv2DTranspose(filters=1, kernel_size=(3,3), strides=(2,2)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.compile(loss= keras.losses.mean_squared_error , optimizer= 'adam' , metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "model = simple_cnn_model()\n",
    "model.summary()\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=15, verbose=1)\n",
    "# # build the model\n",
    "# model = simple_cnn_model()\n",
    "\n",
    "# # Fit the model\n",
    "\n",
    "# t0 = time.time()\n",
    "# log = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=15, verbose=1)\n",
    "# t1 = time.time()\n",
    "# print(t1-t0, \" seconds\")\n",
    "# # Final evaluation of the model\n",
    "# scores = model.evaluate(X_val, y_val, verbose=0)\n",
    "# print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SaltDataset(Dataset):\n",
    "#     def __init__(self, transform, mode, image_size, fold_index, aug_list):\n",
    "\n",
    "#         self.transform = transform\n",
    "#         self.mode = mode\n",
    "#         self.image_size = image_size\n",
    "#         self.aug_list = aug_list\n",
    "\n",
    "#         print('AugList: ')\n",
    "#         print(self.aug_list)\n",
    "\n",
    "#         # change to your path\n",
    "#         self.train_image_path = r\"C:\\Users\\buckf\\Documents\\Practicum_2\\Data\\train\\images\"\n",
    "#         self.train_mask_path = r\"C:\\Users\\buckf\\Documents\\Practicum_2\\Data\\train\\masks\"\n",
    "#         self.test_image_path = r\"C:\\Users\\buckf\\Documents\\Practicum_2\\Data\\test\"\n",
    "\n",
    "#         self.fold_index = None\n",
    "#         self.set_mode(mode, fold_index)\n",
    "\n",
    "\n",
    "#     def set_mode(self, mode, fold_index):\n",
    "#         self.mode = mode\n",
    "#         self.fold_index = fold_index\n",
    "#         print('fold index set: ' + str(fold_index))\n",
    "\n",
    "#         if self.mode == 'train':\n",
    "#             data = pd.read_csv('./data_process/10fold/fold' + str(fold_index) + '_train.csv')\n",
    "#             self.train_list = data['fold']\n",
    "#             self.train_list = [tmp + '.png' for tmp in self.train_list]\n",
    "#             self.num_data = len(self.train_list)\n",
    "\n",
    "#         elif self.mode == 'val':\n",
    "#             data = pd.read_csv('./data_process/10fold/fold' + str(fold_index) + '_valid.csv')\n",
    "#             self.val_list = data['fold']\n",
    "#             self.val_list = [tmp + '.png' for tmp in self.val_list]\n",
    "#             self.num_data = len(self.val_list)\n",
    "\n",
    "#         elif self.mode == 'test':\n",
    "#             self.test_list = read_txt('./data_process/10fold/test.txt')\n",
    "#             self.num_data = len(self.test_list)\n",
    "#             print('set dataset mode: test')\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         if self.fold_index is None:\n",
    "#             print('WRONG!!!!!!! fold index is NONE!!!!!!!!!!!!!!!!!')\n",
    "#             return\n",
    "\n",
    "#         if self.mode == 'train':\n",
    "#             image = cv2.imread(os.path.join(self.train_image_path, self.train_list[index]), 1)\n",
    "#             label = cv2.imread(os.path.join(self.train_mask_path, self.train_list[index]), 0)\n",
    "\n",
    "#         if self.mode == 'val':\n",
    "#             image = cv2.imread(os.path.join(self.train_image_path, self.val_list[index]), 1)\n",
    "#             label = cv2.imread(os.path.join(self.train_mask_path, self.val_list[index]), 0)\n",
    "\n",
    "#         if self.mode == 'test':\n",
    "#             image = cv2.imread(os.path.join(self.test_image_path, self.test_list[index]), 1)\n",
    "#             image_id = self.test_list[index].replace('.png', '')\n",
    "\n",
    "#             if self.image_size == 128:\n",
    "#                 image = resize_and_pad(image, resize_size=101, factor=64)\n",
    "\n",
    "#             image = image.reshape([self.image_size, self.image_size, 3])\n",
    "#             image = np.transpose(image, (2, 0, 1))\n",
    "#             image = image.astype(np.float32)\n",
    "#             image = image.reshape([3, self.image_size, self.image_size])\n",
    "#             image = (image.astype(np.float32) - 127.5) / 127.5\n",
    "#             return image_id, torch.FloatTensor(image)\n",
    "\n",
    "#         is_empty = False\n",
    "#         if np.sum(label) == 0:\n",
    "#             is_empty = True\n",
    "\n",
    "#         if self.mode == 'train':\n",
    "#             image, label = resize_and_random_pad(image, label, resize_size=101, factor=128, limit=(-13, 13))\n",
    "#         else:\n",
    "#             image = resize_and_pad(image, resize_size=101, factor=128)\n",
    "#             label = resize_and_pad(label, resize_size=101, factor=128)\n",
    "\n",
    "#         image = cv2.resize(image, (self.image_size, self.image_size))\n",
    "#         label = cv2.resize(label, (self.image_size, self.image_size))\n",
    "\n",
    "#         if self.mode == 'train':\n",
    "#             if 'flip_lr' in self.aug_list:\n",
    "#                 if random.randint(0, 1) == 0:\n",
    "#                     image = cv2.flip(image, 1)\n",
    "#                     label = cv2.flip(label, 1)\n",
    "\n",
    "#         image = image.reshape([self.image_size, self.image_size, 3])\n",
    "#         label = label.reshape([self.image_size, self.image_size, 1])\n",
    "#         image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "#         image = (image.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "#         label = label.reshape([1, self.image_size, self.image_size])\n",
    "#         label = np.asarray(label).astype(np.float32) / 255.0\n",
    "#         label[label >= 0.5] = 1.0\n",
    "#         label[label < 0.5] = 0.0\n",
    "\n",
    "#         return torch.FloatTensor(image), torch.FloatTensor(label), is_empty\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_foldloader(image_size, batch_size, fold_index, aug_list = None, mode='train'):\n",
    "\n",
    "#     \"\"\"Build and return data loader.\"\"\"\n",
    "#     dataset = SaltDataset(None, mode, image_size, fold_index, aug_list)\n",
    "\n",
    "#     shuffle = False\n",
    "#     if mode == 'train':\n",
    "#         shuffle = True\n",
    "\n",
    "#     data_loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=4, shuffle=shuffle)\n",
    "#     return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
